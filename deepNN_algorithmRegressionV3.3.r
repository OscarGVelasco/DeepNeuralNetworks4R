########################################################################
# Version 3.3:
# Loss Function: Root Mean Squared Deviation
# Activation function: PReLu(Parametric Rectifier Linear Unit)
# Learning Rate: automatically adjusted with Adagrad algorithm (algorithm for gradient-based optimization:
    # It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates 
    # for frequent parameters)
# Optimized version 2017-Jun-27 by Óscar González Velasco - CiC Lab 19
# Deep Neural Network With multiple hidden Layers
########################################################################
### Build the Neural Network structure, values:

deepNeuralNetwork.build <- function(x,y,
                                    HidenLayerNeurons = c(4,4), inputNeurons = 0,
                                    outputNeurons = 0,Ai = 0.25,traindata=data,
                                    drawDNN = 1, random.seed = 1, standarization = NULL){
  source(file = "/home/oscar/Escritorio/deepNN/drawDeepNN.r")
  message(" Loading DNN model parameters...")
  Sys.sleep(1)
  # to make the case reproducible.
  set.seed(random.seed)
  if(!is.numeric(HidenLayerNeurons) || any(HidenLayerNeurons == 0,na.rm = T) || any(is.na(HidenLayerNeurons))){
    stop("Hiden Layer Neurons specification is not valid.")
    return(NULL)
  }
  if(is.list(standarization)){
    x <- 1:(length(x)*(length(standarization)+1))
  }
  # total number of training set
  N <- nrow(traindata)
  # extract the data and label
  # create model
  # number of input features
  ## D = Number of Neurons in input layer
  if(inputNeurons <= 0){
    D <- length(x)
    if(D <= 0)stop("Number of input neurons must be > 0")
  }else{
    D <- inputNeurons  
  }
  # number of categories for classification
  ## K = Number of Neurons in output layer
  if(outputNeurons == 0){
    K <- length(unique(traindata[,y]))
  }else{
    K <- outputNeurons
  }## H = Number of Neurons in the hidden layers
  H <-  HidenLayerNeurons
  message("\n","\tDeep Neural Network Parameters:")
  Sys.sleep(1)
  message(" Inplyr::put Layer: ",D," neurons]",appendLF = F)
  Sys.sleep(1)
  sapply(H,function(x){
    message("---[",x," neurons]",appendLF = F)
    Sys.sleep(0.1)})
  message("---[Output Layer: ",K," neurons.","\n")
  # create and init weights and bias 
  ## On weight Matrices Wi the Nrows corresponds to the number
  ## of Neurons on layer i and the Ncols corresponds to the number 
  ## of Neurons on layer i+1 (the layer where the data is flowing to)
  ## Initial values are randomly generated by rnorm generates random deviates with mean=0.
  ## we will apply a reduction factor 0.01 to obtain small weights
  
  Sys.sleep(3)
  message(" Initializing DNN weights and bias using a gaussian distribution...",appendLF = F)
  Sys.sleep(0.5)
  # Initialize weights with a gaussian distribution
  Winput <- 0.01*matrix(rnorm(D*H[1],sd = sqrt(2.0/D)), nrow=D, ncol=H[1])
  ## On bias matrices bi the Ncols corresponds to the number of Neurons on
  ## layer i+1
  binput <- matrix(0, nrow=1, ncol=H[1])
  A <- matrix(Ai, nrow=1, ncol=H[1])
  d <- list(W = Winput, b = binput, A = Ai)
  Wn <- list(d)
  Whl <- list()
  if(length(H)!=1){
    Whl <- sapply((1:(length(H)-1)),function(i){
      neuronLayer <- H[i]
      neuronLayerNext <- H[i+1]
      Wi <- 0.01*matrix(rnorm(neuronLayer*neuronLayerNext,sd = sqrt(2.0/neuronLayer)), nrow=neuronLayer, ncol=neuronLayerNext)
      bi <- matrix(0, nrow=1, ncol=neuronLayerNext)
      A <- matrix(Ai, nrow=1, ncol=neuronLayerNext)
      d <- list(W = Wi, b = bi, A = Ai)
      Whl[[i+1]] <- list(d)})
    for(i in (2:(length(Whl)+1))){
      Wn[[i]] <- Whl[[i-1]]
    }}
  remove(Whl)
  
  Woutput <- 0.01*matrix(rnorm(H[length(H)]*K, sd = sqrt(2.0/H[length(H)])), nrow=H[length(H)], ncol=K)
  boutput <- matrix(0, nrow=1, ncol=K)
  A <- matrix(Ai, nrow=1, ncol=K)
  Wn[[length(Wn)+1]] <- list(W = Woutput,b = boutput, A = Ai)
  message(" Done.")
  ### Drawing the Neural Network
  if(drawDNN == 1)draw.dnn(NperL = c(D,H,K))
  Sys.sleep(1)
  message(" Returning DNN model.")
  return(Wn)
}

#######################################
## Prediction
deepNeuralNetwork.run <- function(model.trained, data = X.test) {
  # new data, transfer to matrix
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  Wn <- model.trained
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      # Hx[[i]] <- pmax(Hx[[i]], 0.1)
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      #Hx[[i]] <- pmax(Hx[[i]], 0.1)
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }}
  
  ## The Output of the DNN 
  return(y_output)
}

deepNeuralNetwork.predict <- function(model.trained, data = X.test, standarization = NULL) {
  # new data, transfer to matrix
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  message(" Standarizing Data...",appendLF = F)
  if(is.list(standarization)){
    new.data <- t(data)
    new.data <- as.matrix(t(plyr::ldply(lapply(X = standarization,function(x)deepNeuralNetwork.standarize(x = new.data,method = "genecomb",gene = as.character(x))), data.frame)))
    new.data <- as.matrix(cbind(deepNeuralNetwork.standarize(data,method = "r"),new.data))
  }else{
    new.data <- deepNeuralNetwork.standarize(data.matrix(data))
  }
  Sys.sleep(time = 1)
  message(" Done.")
  Sys.sleep(time = 0.5)
  message(" Loading model...",appendLF = F)
  Wn <- model.trained
  Sys.sleep(time = 0.5)
  message(" Done.")
  Sys.sleep(time = 1)
  N <- length(Wn)
  message("\n\t Deep Neural Network Topology:")
  message("> Number of layers: ",N)
  sapply(1:N,function(x){
    message("[Layer ",x," Neurons:",nrow(Wn[[x]]$W),"]--",appendLF = F)
    Sys.sleep(time = 0.5)})
  message("[Output layer Neurons:",ncol(Wn[[N]]$b),"]")
  Sys.sleep(time = 1)
  message("Running model...")
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(new.data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }}
  
  ## The Output of the DNN 
  message("\n> Prediction Completed. Returning results...")
  return(y_output)
}

##################################################
# Training the Deep Neural Network built by build.dnn function, a dataset must be
#    passed in order to train the DNN, x= columns of the data , y = columns of known results that can be predicted using x
deepNeuralNetwork.training <- function(x,y, model = NULL,
                                       traindata=data,
                                       testdata=NULL,
                                       # Max number of iteration steps
                                       iterations=2000,
                                       # Delta loss stop 
                                       minError=1e-2,
                                       # Maximun Error permited on training data to chose best model
                                       maxError=0.05,
                                       # Starting learning rate
                                       lr = 1e-2,
                                       # regularization rate
                                       reg = 1e-3,
                                       # show results every 'display' step
                                       display = 100,
                                       # Set seed for reproducible test
                                       random.seed = 1,
                                       standarization = NULL)
{
  source("/home/oscar/Escritorio/deepNN/linearPlot.r")
  # # ### TEST
  # x=1:200
  # y=201
  # model = model
  # traindata=data
  # testdata=data
  # iterations=2000
  # lr = 1e-2
  # reg = 1e-3
  # ####
  # to make the case reproducible.
  # set.seed(random.seed)
  # 
  # total number of training set
  N <- nrow(traindata)
  # This X Data will be the explanatory data 
  # X.raw <- unname(data.matrix(traindata[,x]))
  X.raw <- data.matrix(traindata[,x])
  if(is.list(standarization)){
    if(!require(plyr)){message("Package plyr not installed.")
      return(NULL)}
    X.raw.t <- t(X.raw)
    X <- as.matrix(t(ldply(lapply(X = standarization,function(x)deepNeuralNetwork.standarize(x = X.raw.t,method = "genecomb",gene = as.character(x))), data.frame)))
    X <- as.matrix(cbind(deepNeuralNetwork.standarize(t(X.raw.t),method = "r"),X))
  }else{X <- t(deepNeuralNetwork.standarize(t(X.raw)))}
  
  # correct categories represented by integer
  # This Y data will be the data to be predicted: Y = F(X) + a
  Y <- traindata[,y]
  
  if(is.factor(Y)) { Y <- as.integer(Y) }
  if(!is.null(testdata) && is.factor(testdata[,y])) { 
    Yprima <- as.integer(testdata[,y])
    SST2 <- sum((Yprima - mean(Yprima))^2)
    SSE2 <- 0.4
    bestEtest <- 0.5
  } else {
      Yprima <- testdata[,y]
      SST2 <- sum((Yprima - mean(Yprima))^2)
      SSE2 <- 0.4
      bestEtest <- 0.5
  }
  X.prima.raw <- testdata[,x]
  if(is.list(standarization)){
    X.prima.t <- t(X.prima.raw)
    X.prima <- as.matrix(t(ldply(lapply(X = standarization,function(x)deepNeuralNetwork.standarize(x = X.prima.t,method = "genecomb",gene = as.character(x))), data.frame)))
    X.prima <- as.matrix(cbind(deepNeuralNetwork.standarize(t(X.prima.t),method = "r"),X.prima))
  }else{X.prima <- t(deepNeuralNetwork.standarize(t(X.prima.raw)))}
  
  # create index for both row and col
  Y.len   <- length(Y)
  # len is the number of different values that Y can take
  Y.set   <- sort(Y)
  Y.index <- cbind(1:N, match(Y, Y.set))
  # The model list with the Weights and Biases
  Wn <- model
  # use all train data to update weights since it's a small dataset
  batchsize <- N
  # Loss error and pastLoss initialization value 
  loss <- 50000
  pastLoss <- 50001
  bestModel <- NULL
  bestModel2 <- NULL
  bestLoss <- 1e6
  bestLoss2 <- 1
  predictionLoss <- 50000
  partialPredict <- 50000
  # Y value dispersion index SST
  SST <- sum((Y - mean(Y))^2)
  ## epsilon: Adagrad value to prevent division by 0
  epsilon <- 1e-8
  # Training the network
  i <- 0
  Hx_function <- list()
  N <- length(Wn)
  X11()
  while(i < iterations && loss > minError ) {
    # iteration index
    i <- i +1
    ## Calculation of the very first layer (INPUTDATA x WEIGHTMatrix + BIAS)
    Hx_function[[1]] <- sweep(X %*% Wn[[1]]$W ,2, Wn[[1]]$b , '+')
   
    # PReLu Function
    Hx_function[[1]] <- pmax(Hx_function[[1]], Wn[[1]]$A * Hx_function[[1]])
    #Hx_function[[1]] <- pmax(Hx_function[[1]], 0.1) ## RElu function
    for(j in 2:(N-1)){
      # forward ....
      # 1 indicate row, 2 indicate col
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 weight matrices length(Wn)==2
      # neurons : ReLU
      # pmax compares parallely each row with a row of 0's
      # in this case, if a value is <0 it will be set to 0
      # this is a fast way to compute the ReLU function (f(x)= max(0,x))
      Hx_function[[j]] <- sweep(Hx_function[[j-1]] %*% Wn[[j]]$W ,2, Wn[[j]]$b , '+')
      # PReLu Function
      Hx_function[[j]] <- pmax(Hx_function[[j]], Wn[[j]]$A * Hx_function[[j]])
      #Hx_function[[j]] <- pmax(Hx_function[[j]], 0.1)  ## RElu function
    }
    y_DNNoutput <- sweep(Hx_function[[N-1]] %*% Wn[[N]]$W, 2, Wn[[N]]$b, '+')
    
    # LOSS FUNCTION:
    # Root Mean Square Error function to map y_DNNoutputs to a value of a variable 
    # Minimize the output error
    probs <- (y_DNNoutput - Y) # y output predicted data minus Y observed data
    data.loss <- sqrt(mean((probs)^2))
    SSE <- sqrt(mean(sum((probs)^2)))
    #SSE <- sum(probs^2) # Summatory before the division by mean ???????**************************************
    Epredicted <- SSE/SST
    reg.loss   <- 0.5*reg* sum(sapply(1:N,function(x){sum(Wn[[x]]$W*Wn[[x]]$W)}))
    loss <- data.loss + reg.loss
    if(loss > 1000){
      cat("\n WARNING:")
      cat("\n","Error loss value has reached:",loss,"\n")
      cat("Check parameters: \n")
      cat("Learning Rate(lr) - actual=",lr,"\n")
      cat("Regularization Rate(reg) - actual=",reg,"\n")
      cat("Returning the LAST Best Model and Best Error \n")
      # Returning the trained model (Wn)
      results <- list(dnn = -1, error = -1, bestDnn = bestModel, bestError = bestLoss)
      rm(Wn,bestModel,dhidden,Hx_function,X,Y)
      gc()
      return(results)
    }
    if(loss < bestLoss){
      bestLoss <- loss
      bestModel <- Wn
    }
    pastLoss <- loss
    ########################
    ## Print partial results
    if(i == 1){
      if(!is.null(testdata)){
        partialResult <- deepNeuralNetwork.run(model.trained = Wn,
                                               data = X.prima)
        partialPredict <- sqrt(mean((partialResult - Yprima)^2))
        e2 <- partialResult - Yprima
        SSE2 <- sum(e2^2)
        Etest <- SSE2/SST2
        if(partialPredict < predictionLoss){ predictionLoss <- partialPredict }
        # if( (((loss / partialPredict)*100) < 80) && (i > 4000)){ 
        #   cat("\n Maximum Loss vs testLoss criteria reached \n")
        #   cat(((loss / partialPredict)*100),"\n")
        #   break()}
      }
      
      cat("\r","Iteration Number:",i," Actual Loss:", loss, " Squared:",SSE,"\n") 
      cat("TLoss Ratio: ",(partialPredict - loss)," Prediction Loss:", predictionLoss, " Partial predict:",partialPredict ,"\n")
      #cat("E1:",standardizedLoss," E2:",standardizedLoss2)
      cat("Error 1:",Epredicted," Error 2:",Etest,"\n")
      cat("\n Log2(E1/E2):",log2(Epredicted/Etest),"\n")
      cat("##################################################### \n")
      print(mplot_lineal(tag = Y,score = y_DNNoutput))
      #qqplot(y_DNNoutput,Y, xlab = "Predicted", ylab = "Observed", main = "Observed Values vs Predicted Values")
      #mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),sep = " "))
      #abline(0,1) 
    }
    if( i %% display == 0){
      #######################################
      ## Check for Overfitting and early stop
      if(!is.null(testdata)){
        partialResult <- deepNeuralNetwork.run(model.trained = Wn,
                                               data = X.prima)
        partialPredict <- sqrt(mean((partialResult - Yprima)^2))
        e2 <- partialResult - Yprima
        SSE2 <- sum(e2^2)
        Etest <- SSE2/SST2
        if(partialPredict < predictionLoss){ predictionLoss <- partialPredict }
        # if( (((loss / partialPredict)*100) < 80) && (i > 4000)){ 
        #   cat("\n Maximum Loss vs testLoss criteria reached \n")
        #   cat(((loss / partialPredict)*100),"\n")
        #   break()}
      }
      if((Epredicted < maxError) & (Etest < bestEtest)){
        bestEtest <- Etest
        bestLoss2 <- Etest
        bestModel2 <- Wn
      }
      cat("\r","Iteration Number:",i," Actual Loss:", loss, " Squared:",SSE,"\n") 
      cat("TLoss Ratio: ",(partialPredict - loss)," Prediction Loss:", predictionLoss, " Partial predict:",partialPredict ,"\n")
      #cat("E1:",standardizedLoss," E2:",standardizedLoss2)
      cat("Error 1:",Epredicted," Error 2:",Etest,"\n")
      cat("\n Log2(E1/E2):",log2(Epredicted/Etest),"\n")
      cat("##################################################### \n")
      title <- paste("Deep Neural Network Training - Iteration Number:",i,"[",round((i/iterations)*100,digits = 1),"%]")
      subtitle <- "Regresion Model parcial prediction."
      print(mplot_lineal(tag = Y,score = y_DNNoutput,title = title,subtitle = subtitle))
      # qqplot(y_DNNoutput,Y, xlab = "Predicted", ylab = "Observed", main = "Observed Values vs Predicted Values")
      # mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),sep = " "))
      # abline(0,1)
      # 
      }else{
     # if( i %% 100 == 0){
     #    qqplot(y_DNNoutput,Y)
     #    abline(0,1)
     #  }
      cat("\r","Iteration Number:",i," Actual Loss:", loss)
    }
    ############################
    # Backpropagation Algorithm: updating of Weights and Bias
    # We start by updating and calculating the values for the output layer -> j layer -> inputLayer
    delta_y <- 2*probs
    delta_y <- delta_y / batchsize
    ## Output Layer (Layer N-1)
    derivativeWeights <- crossprod(Hx_function[[N-1]],delta_y) 
    derivativeBias <- colSums(delta_y)
    # Wn[[N]] Here corresponds with layer l-1 weights
    dhidden <- tcrossprod(delta_y,Wn[[N]]$W)
    
    # Applying the derivative ReLu 
    dPrelu <- pmin(dhidden,0)
    dhidden[Hx_function[[N-1]] <= 0] <- Wn[[N-1]]$A * dhidden[Hx_function[[N-1]] <= 0]
    
    adagradW <- (lr / sqrt(mean(derivativeWeights^2)+epsilon))
    adagradB <- (lr / sqrt(mean(derivativeBias^2)+epsilon))
    Wn[[N]]$W <- Wn[[N]]$W - adagradW * derivativeWeights
    Wn[[N]]$b <- Wn[[N]]$b - adagradB * derivativeBias
    ## NO Wn$A on N Layer as there is no activation function on
    #     the last layer, only loss function
     
    for(j in (N-1):2){
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 waight matrices length(Wn)==2
      ## The i intermediate Layers Backpropagation:
      derivativeWeights <- crossprod(Hx_function[[j-1]],dhidden) 
      derivativeBias <- colSums(dhidden)
      derivativePrelu <- sum(colSums(dPrelu))
      adagradW <- lr / sqrt(mean(derivativeWeights^2)+epsilon)#%*%derivativeWeights
      adagradB <- lr / sqrt(mean(derivativeBias^2)+epsilon)
      adagradA <- (lr / sqrt(mean(colSums(dPrelu)^2)+epsilon))
      
      #Calculate Delta for the next gradient computation:
      dhidden <- tcrossprod(dhidden,Wn[[j]]$W)
      dPrelu <- pmin(dhidden,0)
      dhidden[Hx_function[[j-1]] <= 0] <- Wn[[j-1]]$A * dhidden[Hx_function[[j-1]] <= 0]
      
      # Updating of Weights, Bias and Alfa-PReLu value:
      Wn[[j]]$W <- Wn[[j]]$W - adagradW * derivativeWeights
      Wn[[j]]$b <- Wn[[j]]$b - adagradB * derivativeBias
      Wn[[j]]$A <- 0.9 * Wn[[j]]$A + adagradA * derivativePrelu # 0.9:momentum of PReLu Ai parameter
    }
    ## Backpropagation for the Input Layer:
    derivativeWeights <- crossprod(X,dhidden)
    derivativeBias <- colSums(dhidden)
    derivativePrelu <- sum(colSums(dPrelu))
    # Updating parameters of the Input layer
    adagradW <- (lr / sqrt(mean(derivativeWeights^2)+epsilon))#%*%derivativeWeights
    adagradB <- (lr / sqrt(mean(derivativeBias^2)+epsilon))
    adagradA <- (lr / sqrt(mean(colSums(dPrelu)^2)+epsilon))
    Wn[[1]]$W <- Wn[[1]]$W - adagradW * derivativeWeights
    Wn[[1]]$b <- Wn[[1]]$b - adagradB * derivativeBias
    Wn[[1]]$A <- 0.9 * Wn[[1]]$A + adagradA * derivativePrelu # 0.9:momentum of PReLu Ai parameter
  }
  #############################################
  # Final results, statistics and free memory #
 
  cat("Total Data Loss MSE:", loss, "\n")
  residuals <- Y - y_DNNoutput
  res.std <- (residuals - mean(residuals))/sd(residuals)
  dev.set(1)
  plot(y_DNNoutput,res.std,
       xlab = "Predicted Values", ylab = "Standarized Residuals",
       main = "Residuals Plot vs Predicted Values")
  mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),sep = " "))
  abline(0,0)
  # Returning the trained model (Wn)
  results <- list(dnn = Wn, error = loss, bestDnn = bestModel, bestError = bestLoss,bestDnn2 = bestModel2, bestError2 = bestLoss2)
  rm(Wn,bestModel,dhidden,Hx_function,X,Y)
  gc()
  return(results)
}

### Standarizing gene signal by targeted gene ENSG00000075856 stable in Hippocampus
### Standarizing gene signal by targeted gene ENSG00000124214 stable in Cortex
deepNeuralNetwork.standarize <- function(x = NULL, method = "gene",gene = "ENSG00000075856"){
  if(is.null(x))stop("Standarizing data: ERROR, data is NULL.")
  if(!is.numeric(x))stop("Standarizing data: ERROR, trying to standarize non-numeric data.")
  if(is.null(dim(x)))x<-t(as.data.frame(x))
  if( (summary(apply(x,MARGIN = 1,FUN = sd))[3] == 1) && (summary(apply(x,MARGIN = 1,FUN = sd))[4]== 1) ){
    warning("The Data is already standarized (Standar Deviation = 1) \n*None standarization applied.")
    return(x)}
  if(!method %in% c("s","r","gene","genecomb")){
    message("Invalid Method \"",method,"\": \n - \"s\" for standar Z-score using the mean and SD \n - \"r\" for robust Z-score using the median and MAD \n - \"gene\" for specific-gene Z-score using the gene as centroid - |n \"genecomb\" for specific-gene Z-score using the gene as centroid and  sample-gene combined standar deviation")
    warning("choose method = \"-\"",immediate. = T)
  }
  ## standard Z-Score using mean and SD:
  if(method == "s"){
    message("Standarizing data [by row] using *mean standar deviation Z-score...",appendLF = F)
    MX.standarized <- t(apply(X = x,MARGIN = 1,FUN = function(y){
      (y - mean(y))/sd(y)}))
    message(" Completed.\n")
    return(MX.standarized)
  } 
  #
  if(method == "r"){
    message("Standarizing data [by row] using *median absolute deviation(MAD) Z-score...",appendLF = F)
    rowmed <- apply(x, 1, median)
    rowmad <- apply(x, 1, mad)  # median absolute deviation
    rv <- sweep(x, 1, rowmed,"-")  #subtracting median expression
    rv <- sweep(rv, 1, rowmad, "/")  # dividing by median absolute deviation
    message(" Completed.")
    return(rv)
  }
  if(method == "gene"){
    message("Standarizing data [by column] using *specific-gene Z-score...",appendLF = F)
    MX.standarized <- apply(X = x,MARGIN = 2,FUN = function(y){
      (y - y[match(gene,names(y))])/sd(y)})
    message(" Completed.\n")
    return(MX.standarized)
    # rv <- apply(x,1,function(x){x-x[gene]})
    #rv <- sweep(x, 1, gene.signal,"-")  #subtracting median expression
    # rv <- sweep(rv, 1, gene.sd, "/")  # dividing by median absolute deviation
  }
  if(method == "genecomb"){
    message("Standarizing data using *specific-gene Z-score with SD(sample,gene) combined...",appendLF = F)
    gene.mean <- mean(x[match(gene,rownames(x)),])
    gene.n <- length(x[match(gene,rownames(x)),])
    gene.sd <- sd(x[match(gene,rownames(x)),])
    MX.standarized <- apply(X = x,MARGIN = 2,FUN = function(y){
      sample.mean <- mean(y)
      sample.n <- length(y)
      sample.sd <- sd(y)
      combined.mean <- ((gene.n*gene.mean)+(sample.n*sample.mean))/(gene.n+sample.n)
      combined.sd <- sqrt( ( ((gene.n-1)*(gene.sd^2))+((sample.n-1)*(sample.sd^2))+gene.n*((gene.mean-combined.mean)^2)+sample.n*((sample.mean-combined.mean)^2) )/ (gene.n+sample.n-1))
      return((y - y[match(gene,names(y))])/combined.sd)})
    message(" Completed.\n")
    return(MX.standarized)
  }
}

deepNeuralNetwork.save <- function(wn = NULL,
                                   nameoffile = paste("DNN.model.",format(Sys.time(),"%d.%m.%Y.%H.%M.%S"),sep = "")){
  
  assign(nameoffile,wn)
  save(list = nameoffile,file = paste(nameoffile,".RData",sep = ""))
  
  return(nameoffile)
}

# 
# ########################################################################
# # testing
# #######################################################################
# set.seed(1)
# # # 0. EDA
# # summary(iris)
# # plot(iris)
# # 1. split data into test/train
# samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
# # 2. Create the Neural Network specifying the layers
# model <- deepNeuralNetwork.build(x=2:5,y=1,HidenLayerNeurons = c(30,10,10,10), outputNeurons = 1,
#                                  traindata=iris[samp,],random.seed = 1, drawDNN = 0)
# normIris <- sapply(iris[,-1],FUN = function(x){
#   print(class(x))
#   if(is.numeric(x)){
#     y <- (x-min(x))/(max(x)-min(x))
#     return(y)
#   }else{
#     return(x)
#   }
# })
# data <- data.frame(iris[,1],normIris)
# # 3. train model
# timeNN <- system.time(
#   model.trained <- deepNeuralNetwork.training(x=2:5,y=1, model = model, traindata=data[samp,],
#                                               testdata=data[-samp,], lr = 0.001, reg = 0.0001
#                                               ,iterations = 200000, display=1000))
# scale(iris[samp,-5],
#       center=TRUE, scale=FALSE)
# # 4. prediction
# # NOTE: if the predict is factor, we need to transfer the number into class manually.
# #       To make the code clear, I don't write this change into predict.dnn function.
# results <- deepNeuralNetwork.run(model.trained = model.trained$bestDnn,
#                                  data = data[-samp, -1])
# # Total Data Loss MSE: 0.2946946 with 10 neurons and 4000 iterations
# # 5. verify the results
# #table(iris[-samp,1], results)
# accuracy <- mean((iris[-samp,1]-results))
# 
# as.numeric(iris[-samp,1])[1:10] - results[1:10]
# cor(as.numeric(iris[-samp,1]),results)
# sqrt(mean((as.numeric(iris[-samp,1]) - results)^2))
# #0.6773504
# #          labels.dnn
# #            1  2  3
# #setosa     25  0  0
# #versicolor  0 24  1
# #virginica   0  0 25
# # 6. accuracy
# mean(as.integer(iris[-samp, 5]) == results)
# # 0.98
# 
# 
########################################
# Calculating Age with expression data
# setwd("/data/NGSdata/GSE25219_humanBrain_spatioTemporal/RData/")
# load("codingrmaMX.RData")
# load("HumanSpatioTempTableResultsHip.RData")
# 
# targetGenes <- rownames(gammaTableNcx100k[1:400,])
# samp2 <- c(sample(1:200,100), sample(200:400,100), sample(400:599,100))
# data <- t(rmaMX[targetGenes,samp2])
# data <- cbind(data, age= donorsDataFinal[rownames(data),]$Age)
# testdata <- t(rmaMX[targetGenes,-samp2])
# testdata <- cbind(testdata, age= donorsDataFinal[rownames(testdata),]$Age)
# 
# model <- deepNeuralNetwork.build(x=1:length(targetGenes),y=ncol(data), outputNeurons = 1,
#                                  HidenLayerNeurons = c(60,40,20,20,10,10,10,10,10,10,10,10,10,10),traindata=data,
#                                  random.seed = 1, drawDNN = 0)
# # 3. train model
# timeNN <- system.time(
#   model.trained <- deepNeuralNetwork.training(x=1:length(targetGenes),y=ncol(data), model = model,
#                                               traindata=data, testdata=testdata, 
#                                               iterations  = 100000, lr = 0.0001, 
#                                               reg = 0.0001, display=1000))
# predictData <- t(rmaMX[targetGenes,-samp2])
# predictData <- cbind(predictData, age= donorsDataFinal[rownames(predictData),]$Age)
# results <- deepNeuralNetwork.run(model.trained = model.trained$bestDnn, data = predictData[,-301])
# cor(predictData[,301], results)
# cor(donorsDataFinal[rownames(results),]$Age,results[,1])
# cor(predictData[,1],resftp://cicblade.dep.usal.es:5100/data/NGSdata/deepNNsults[,1])
# n <- 15 
# (donorsDataFinal[rownames(results),]$Age)[n]
# round(results[n,1])
# residuals <- donorsDataFinal[rownames(results),]$Age - results[,1]
# res.std <- (residuals - mean(residuals))/sd(residuals)
# boxplot(summary(donorsDataFinal[rownames(results),]$Age - results[,1]), title = "Dispersion")
# sqrt(mean((donorsDataFinal[rownames(results),]$Age - round(results[,1]))^2))
# compare <- cbind(round(results), realData <- donorsDataFinal[rownames(results),]$Age)
# plot(results,res.std)
# plot(compare)
# #####
# # Best Results:
# # HiddenLayer = 50,40,40,30   it= 200.000 lr = 0.00001
# # Total Data Loss MSE: 5.543729
# # Correlation = 0.9608447  Squared Err: 6.17363 
# # > summary(donorsDataFinal[rownames(results),]$Age - results[,1])
# # Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# # -17.070  -2.981   1.121   1.046   5.019  31.040
# 
# 
# 
# ############################################
# ## Calculating AD STAGE with expression data
# 
# targetGenes <- brainGenes[brainGenes %in% rownames(ADinterrogation_CodingrmaMX)]
# samp2 <-sample(1:161,80)
# data <- t(ADinterrogation_CodingrmaMX[targetGenes,samp2])
# data <- cbind(data, ad= ADinterrogation_Phenodata[rownames(data),]$disease_state)
# testdata <- t(ADinterrogation_CodingrmaMX[targetGenes,-samp2])
# testdata <- cbind(testdata, ad= as.factor(ADinterrogation_Phenodata[rownames(testdata),]$disease_state))
# 
# model <- deepNeuralNetwork.build(x=1:length(targetGenes),y=ncol(data), outputNeurons = 1,
#                                  HidenLayerNeurons = c(160,70),traindata=data,
#                                  random.seed = 1, drawDNN = 0)
# # 3. train model
# timeNN <- system.time(
#   model.trained <- deepNeuralNetwork.training(x=1:length(targetGenes),y=ncol(data), model = model,
#                                               traindata=data, testdata=testdata, 
#                                               iterations  = 100000, lr = 0.0001, 
#                                               reg = 0.0001, display=1000))
# predictData <- t(ADinterrogation_CodingrmaMX[targetGenes,-samp2])
# predictData <- cbind(predictData, ad= ADinterrogation_Phenodata[rownames(predictData),]$disease_state)
# results <- deepNeuralNetwork.run(model.trained = model.trained$bestDnn, data = predictData[,-113])
# 
# cor(predictData[,ncol(predictData)], round(results))
# cor(as.integer(ADinterrogation_Phenodata[rownames(predictData),]$disease_state),results[,1])
# cor(predictData[,1],results[,1])
# n <- 25 
# as.integer((ADinterrogation_Phenodata[rownames(predictData),]$disease_state)[n])
# round(results[n,1])
# residuals <- donorsDataFinal[rownames(results),]$Age - results[,1]
# res.std <- (residuals - mean(residuals))/sd(residuals)
# boxplot(summary(donorsDataFinal[rownames(results),]$Age - results[,1]), title = "Dispersion")
# sqrt(mean((donorsDataFinal[rownames(results),]$Age - round(results[,1]))^2))
# compare <- cbind(round(results), realData <- as.integer((ADinterrogation_Phenodata[rownames(predictData),]$disease_state)))
# plot(results,res.std)
# plot(compare)
# #
# blalockMX2[,1:8]# Blalock Severe alzheimer Stage
# predictData <- t(blalockMX2[targetGenes,1:8])
# results <- deepNeuralNetwork.run(model.trained = model.trained$bestDnn, data = predictData)
# 
# #####