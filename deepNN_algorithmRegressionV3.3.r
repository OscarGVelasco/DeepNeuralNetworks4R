########################################################################
# Version 3.3:
# Loss Function: Root Mean Squared Deviation
# Activation function: PReLu(Parametric Rectifier Linear Unit)
# Learning Rate: automatically adjusted with Adagrad algorithm (algorithm for gradient-based optimization:
# It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates 
# for frequent parameters)
# Optimized version 2018-Jun-27 by Óscar González Velasco - CiC Lab 19
# Deep Neural Network With multiple hidden Layers
########################################################################

### Build the Neural Network structure and initial values:
deepNeuralNetwork.build <- function(x,y,
                                    HidenLayerNeurons = c(4,4), inputNeurons = 0,
                                    outputNeurons = 0,Ai = 0.25,traindata=data,
                                    drawDNN = FALSE, random.seed = 1, standarization = "r"){
  source(file = "./drawDeepNN.r")
  message(" Loading DNN model parameters...")
  Sys.sleep(1)
  # to make the case reproducible.
  if(random.seed)set.seed(random.seed)
  if(!is.numeric(HidenLayerNeurons) || any(HidenLayerNeurons == 0,na.rm = T) || any(is.na(HidenLayerNeurons))){
    stop("Hiden Layer Neurons specification is not valid.")
    return(NULL)
  }
  # total number of training set
  N <- nrow(traindata)
  
  # D = Number of Neurons in input layer
  if(inputNeurons <= 0){
    # if number of input neurons is not set - we determine it by the dimension of the input data matrix
    if(is.list(standarization)){
      # Calculating the number of variables resulting from the target-gene standarization:
      #     number of input variables = (number of rows of input data * number of target-genes) - number of target genes #-> removing the genes itself as they are 0's
      x <- 1:((length(x)*(length(standarization)+1))-length(standarization))
    }
    D <- length(x)
    if(D <= 0)stop("Number of input neurons must be > 0")
  }else{
    # special case in which user defines the input number of neurons
    D <- inputNeurons  
  }
  ## K = Number of Neurons in output layer or number of categories for classification
  if(outputNeurons == 0){
    K <- length(unique(traindata[,y]))
  }else{
    K <- outputNeurons
  }
  ## H = Number of Neurons in the hidden layers
  H <-  HidenLayerNeurons
  message("\n","\tDeep Neural Network Parameters:")
  Sys.sleep(1)
  message(" Input Layer: ",D," neurons]",appendLF = F)
  Sys.sleep(0.2)
  sapply(H,function(x){
    message("---[",x," neurons]",appendLF = F)
    Sys.sleep(0.2)})
  message("---[Output Layer: ",K," neurons.","\n")
  Sys.sleep(1)

  # create and init weights and bias 
  ## On weight Matrices Wi the Nrows corresponds to the number
  ## of Neurons on layer i and the Ncols corresponds to the number 
  ## of Neurons on layer i+1 (the layer where the data is flowing to)
  #
  ## Initial values are randomly generated by rnorm: generates random deviates with mean=0.
  ## we will apply a reduction factor 0.01 to obtain small weights
  message(" Initializing DNN weights and bias using a gaussian distribution...",appendLF = F)
  Sys.sleep(0.5)
  # Initialize weights with a gaussian distribution
  Winput <- 0.01*matrix(rnorm(D*H[1],sd = sqrt(2.0/D)), nrow=D, ncol=H[1])
  ## On bias matrices bi the Ncols corresponds to the number of Neurons on
  ## layer i+1
  binput <- matrix(0, nrow=1, ncol=H[1])
  A <- matrix(Ai, nrow=1, ncol=H[1])
  d <- list(W = Winput, b = binput, A = Ai)
  Wn <- list(d)
  Whl <- list()
  if(length(H)!=1){
    Whl <- sapply((1:(length(H)-1)),function(i){
      neuronLayer <- H[i]
      neuronLayerNext <- H[i+1]
      Wi <- 0.01*matrix(rnorm(neuronLayer*neuronLayerNext,sd = sqrt(2.0/neuronLayer)), nrow=neuronLayer, ncol=neuronLayerNext)
      bi <- matrix(0, nrow=1, ncol=neuronLayerNext)
      A <- matrix(Ai, nrow=1, ncol=neuronLayerNext)
      d <- list(W = Wi, b = bi, A = Ai)
      Whl[[i+1]] <- list(d)})
    for(i in (2:(length(Whl)+1))){
      Wn[[i]] <- Whl[[i-1]]
    }}
  remove(Whl)
  
  Woutput <- 0.01*matrix(rnorm(H[length(H)]*K, sd = sqrt(2.0/H[length(H)])), nrow=H[length(H)], ncol=K)
  boutput <- matrix(0, nrow=1, ncol=K)
  A <- matrix(Ai, nrow=1, ncol=K)
  Wn[[length(Wn)+1]] <- list(W = Woutput,b = boutput, A = Ai)
  message(" Done.")
  ### Drawing the Neural Network
  if(drawDNN)draw.dnn(NperL = c(D,H,K))
  message(" Returning DNN model.")
  # Creating the DeepNNModel object: this object will store all information, structure and data of the Neural Network
  DeepNNModel <- setClass(Class = "DeepNNModel", slots = list(dnn = "list", error = "numeric", bestDnn = "list",
                                                              bestError = "numeric",bestDnn2 = "list",
                                                              bestError2 = "numeric", dnn.structure = "vector",
                                                              training = "list"))
  DeepNNModel <- DeepNNModel(dnn = Wn, dnn.structure = c(D,H,K))
  return(DeepNNModel)
}

#########################################
## Partial Prediction for in-training use
## - A partial prediction over the test data is calculated
##   to measure overfitting.
deepNeuralNetwork.run <- function(model.trained, data = X.test) {
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  Wn <- model.trained
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
      # Hx[[i]] <- pmax(Hx[[i]], 0.1) # ReLU Function implementation with minimun constant 0.1
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
      # Hx[[i]] <- pmax(Hx[[i]], 0.1) # ReLU Function implementation with minimun constant 0.1
    }}
  ## The Output of the DNN 
  return(y_output)
}

deepNeuralNetwork.predict <- function(model.trained, data = X.test, standarization = NULL) {
  # Prediction over unseen data
  # - Same standarization as in training must be done
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  message(" Standarizing Data...",appendLF = F)
  if(is.list(standarization)){
    new.data <- deepNeuralNetwork.standarizegenescore(x = data,gene.list = standarization)
    new.data <- t(new.data)
    
  }else{
    new.data <- deepNeuralNetwork.standarize(data.matrix(data))
    new.data <- t(new.data)
  }
  Sys.sleep(time = 1)
  message(" Done.")
  message(" Loading model...",appendLF = F)
  Wn <- model.trained
  Sys.sleep(time = 0.5)
  message(" Done.")
  N <- length(Wn)
  message("\n\t Deep Neural Network Topology:")
  Sys.sleep(time = 0.2)
  message("> Number of layers: ",N)
  sapply(1:N,function(x){
    message("[Layer ",x," Neurons:",nrow(Wn[[x]]$W),"]--",appendLF = F)
    Sys.sleep(time = 0.2)})
  message("[Output layer Neurons:",ncol(Wn[[N]]$b),"]")
  Sys.sleep(time = 1)
  message("Running model...")
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(new.data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }}
  
  ## The Output of the DNN 
  message("\n> Prediction Completed.")
  return(y_output)
}

##################################################
# Training the Deep Neural Network built by build.dnn function, a dataset must be
#    passed in order to train the DNN, x= columns of the data , y = columns of known results that can be predicted using x
deepNeuralNetwork.training <- function(x,y, model = NULL,
                                       traindata=data,
                                       testdata=NULL,
                                       # Max number of iteration steps
                                       iterations=2000,
                                       # Delta loss stop 
                                       minError=1e-2,
                                       # Maximun Error permited on training data to chose best model
                                       maxError=0.05,
                                       # Starting learning rate
                                       lr = 1e-2,
                                       # regularization rate
                                       reg = 1e-3,
                                       # show results every 'display' step
                                       display = 100,
                                       # Set seed for reproducible test
                                       random.seed = 1,
                                       standarization = NULL,
                                       # savePlotIteration = save the plot of each iteration as a .png image?
                                       savePlotIteration = FALSE)
{
  source("./linearPlot.r")
  # traindata <- data
  # total number of training set
  if(!class(model) == "DeepNNModel"){error("The Deep NN Model is not of class: \"DeepNNModel\"")}
  else{model@training <- list(iterations = iterations,learning.rate = lr, regularization.rate = reg)}
  N <- ncol(traindata)
  # This X Data will be the explanatory data 
  X.raw.t <- t(traindata)
  X.raw.t <- data.matrix(X.raw.t[,x])
  if(is.list(standarization)){
    data = traindata[x,]
    X <- deepNeuralNetwork.standarizegenescore(x = data,gene.list = standarization)
    X <- t(X)
  }else{
    X <- deepNeuralNetwork.standarize(traindata[x,])
    X <- t(X)
  }
  print(dim(X))
  # correct categories represented by integer
  # This Y data will be the data to be predicted: Y = F(X) + a
  Y <- traindata[y,]
  
  # Test data for overfitting purposes
  if(is.factor(Y)) { Y <- as.integer(Y) }
  if(!is.null(testdata) && is.factor(testdata[y,])){ 
    Yprima <- as.integer(testdata[y,])
    batchsize.test <- ncol(testdata)
    SST2 <- sum((Yprima - mean(Yprima))^2)
    SSE2 <- 0.4
    bestEtest <- 0.5
  } else {
      batchsize.test <- ncol(testdata)
      Yprima <- testdata[y,]
      SST2 <- sum((Yprima - mean(Yprima))^2)
      SSE2 <- 0.4
      bestEtest <- 0.5
  }
  X.prima.raw <- t(testdata)
  X.prima.t <- X.prima.raw[,x]
  if(is.list(standarization)){
    data = testdata[x,]
    X.prima <- deepNeuralNetwork.standarizegenescore(x = data,gene.list = standarization)
    X.prima <- t(X.prima)
  }else{
    X.prima <- deepNeuralNetwork.standarize(testdata[x,])
    X.prima <- t(X.prima)
    }
  
  # create index for both row and col
  Y.len   <- length(Y)
  # len is the number of different values that Y can take
  Y.set   <- sort(Y)
  Y.index <- cbind(1:N, match(Y, Y.set))
  # The model list with the Weights and Biases
  Wn <- model@dnn
  # use all train data to update weights since it's a small dataset
  batchsize <- N
  # Loss error and pastLoss initialization value 
  loss <- 50000
  pastLoss <- 50001
  bestModel <- NULL
  bestModel2 <- NULL
  bestLoss <- 1e6
  bestLoss2 <- 1
  predictionLoss <- 50000
  partialPredict <- 50000
  # Y value dispersion index SST
  SST <- sum((Y - mean(Y))^2)
  ## epsilon: Adagrad value to prevent division by 0
  epsilon <- 1e-8
  # Training the network
  i <- 0
  Hx_function <- list()
  N <- length(Wn)
  X11(width = 10,height = 10)
  while(i < iterations && loss > minError ) {
    # iteration index
    i <- i +1
    ## Calculation of the very first layer (INPUTDATA x WEIGHTMatrix + BIAS)
    Hx_function[[1]] <- sweep(X %*% Wn[[1]]$W ,2, Wn[[1]]$b , '+')
   
    # PReLu Function
    Hx_function[[1]] <- pmax(Hx_function[[1]], Wn[[1]]$A * Hx_function[[1]])
    #Hx_function[[1]] <- pmax(Hx_function[[1]], 0.1) ## RElu function
    for(j in 2:(N-1)){
      # forward ....
      # 1 indicate row, 2 indicate col
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 weight matrices length(Wn)==2
      # neurons : ReLU
      # pmax compares parallely each row with a row of 0's
      # in this case, if a value is <0 it will be set to 0
      # this is a fast way to compute the ReLU function (f(x)= max(0,x))
      Hx_function[[j]] <- sweep(Hx_function[[j-1]] %*% Wn[[j]]$W ,2, Wn[[j]]$b , '+')
      # PReLu Function
      Hx_function[[j]] <- pmax(Hx_function[[j]], Wn[[j]]$A * Hx_function[[j]])
      #Hx_function[[j]] <- pmax(Hx_function[[j]], 0.1)  ## RElu function
    }
    y_DNNoutput <- sweep(Hx_function[[N-1]] %*% Wn[[N]]$W, 2, Wn[[N]]$b, '+')
    
    # LOSS FUNCTION:
    # Root Mean Square Error function to map y_DNNoutputs to a value of a variable 
    # Minimize the output error
    probs <- (y_DNNoutput - Y) #/ batchsize # y output predicted data minus Y observed data
    data.loss <- sqrt(mean((probs)^2))
    SSE <- sqrt(mean(sum((probs)^2)))
    #SSE <- sum(probs^2) # Summatory before the division by mean ???????**************************************
    Epredicted <- SSE/SST
    reg.loss   <- reg* (sum(sapply(1:N,function(x){sum(Wn[[x]]$W*Wn[[x]]$W)})) / batchsize )
    probs <- probs+reg.loss
    loss <- sum(data.loss,reg.loss,na.rm = T)
    if(loss > 1000){
      cat("\n WARNING:")
      cat("\n","Error loss value has reached:",loss,"\n")
      cat("Check parameters: \n")
      cat("Learning Rate(lr) - actual=",lr,"\n")
      cat("Regularization Rate(reg) - actual=",reg,"\n")
      cat("Returning the LAST Best Model and Best Error \n")
      # Returning the trained model (Wn)
      model@dnn <- Wn
      model@error <- loss
      model@bestDnn <- bestModel
      model@bestError <- bestLoss
      if(is.null(bestModel2)){model@bestDnn2 <- as.list(bestModel2)}
      else{model@bestDnn2 <- bestModel2}
      model@bestError2 <- bestLoss2
      rm(Wn,bestModel,dhidden,Hx_function,X,Y)
      gc()
      return(model)
    }
    if(loss < bestLoss){
      bestLoss <- loss
      bestModel <- Wn
    }
    pastLoss <- loss
    ########################
    ## Print partial results
    if(i == 1){
      if(!is.null(testdata)){
        print(dim(X.prima))
        partialResult <- deepNeuralNetwork.run(model.trained = Wn,
                                               data = X.prima)
        partialPredict <- (sqrt(mean((partialResult - Yprima)^2)))/ batchsize.test
        e2 <- partialResult - Yprima
        SSE2 <- sum(e2^2)
        Etest <- SSE2/SST2
        if(partialPredict < predictionLoss){ predictionLoss <- partialPredict }
        }
      
      cat("\r","Iteration Number:",i," Actual Loss:", loss, " Squared:",SSE,"\n") 
      cat("TLoss Ratio: ",(partialPredict - loss)," Prediction Loss:", predictionLoss, " Partial predict:",partialPredict ,"\n")
      cat("Error 1:",Epredicted," Error 2:",Etest,"\n")
      cat("\n Log2(E1/E2):",log2(Epredicted/Etest),"\n")
      cat("##################################################### \n")
      }
    if( i %% display == 0){
      #######################################
      ## Check for Overfitting and early stop
      if(!is.null(testdata)){
        partialResult <- deepNeuralNetwork.run(model.trained = Wn,
                                               data = X.prima)
        partialPredict <- (sqrt(mean((partialResult - Yprima)^2)))/ batchsize.test
        e2 <- partialResult - Yprima
        SSE2 <- sum(e2^2)
        Etest <- SSE2/SST2
        if(partialPredict < predictionLoss){ predictionLoss <- partialPredict }
        }
      if((Epredicted < maxError) & (Etest < bestEtest)){
        bestEtest <- Etest
        bestLoss2 <- Etest
        bestModel2 <- Wn
      }
      cat("\r","Iteration Number:",i," Actual Loss:", loss, " Squared:",SSE,"\n") 
      cat("TLoss Ratio: ",(partialPredict - loss)," Prediction Loss:", predictionLoss, " Partial predict:",partialPredict ,"\n")
      cat("Error 1:",Epredicted," Error 2:",Etest,"\n")
      cat("\n Log2(E1/E2):",log2(Epredicted/Etest),"\n")
      cat("##################################################### \n")
      title <- paste("Deep Neural Network Training - Iteration Number:",i,"[",round((i/iterations)*100,digits = 1),"%]")
      subtitle <- "Regresion Model parcial prediction."
      if(savePlotIteration){
        # Should the plot image for each epoque-iteration be saved? TRUE -> png image with the iteration number as name
        print(mplot_lineal(tag = Y,score = y_DNNoutput,title = title,subtitle = subtitle,save = T,file_name = paste("regression.model.iteration.",as.character(i),".png",sep = ""),subdir = "/home/oscar/Escritorio/DeepNeuralNetworks4R/images/"))
        }
      else{
        print(mplot_lineal(tag = Y,score = y_DNNoutput,title = title,subtitle = subtitle))
        }
      }else{
        cat("\r","Iteration Number:",i," Actual Loss:", loss)
        }
    ############################
    # Backpropagation Algorithm: updating of Weights and Bias
    # We start by updating and calculating the values for the output layer -> j layer -> inputLayer
    
    # We apply the derivative function of the Root Mean Square Error formula:
    delta_y <- 2*(probs)
    # Output Layer (Layer N-1)
    derivativeWeights <- crossprod(Hx_function[[N-1]],delta_y)
    derivativeBias <- colSums(delta_y)
    # Wn[[N]] Here corresponds with layer l-1 weights
    dhidden <- tcrossprod(delta_y,Wn[[N]]$W)
    
    # Applying the derivative function of the ReLu formula
    dPrelu <- pmin(dhidden,0)
    dhidden[Hx_function[[N-1]] <= 0] <- Wn[[N-1]]$A * dhidden[Hx_function[[N-1]] <= 0]
    
    adagradW <- (lr / sqrt(mean(derivativeWeights^2)+epsilon))
    adagradB <- (lr / sqrt(mean(derivativeBias^2)+epsilon))
    Wn[[N]]$W <- Wn[[N]]$W - adagradW * derivativeWeights
    Wn[[N]]$b <- Wn[[N]]$b - adagradB * derivativeBias
    ## NO Wn$A on N Layer as there is no activation function on
    #     the last layer, only loss function
     
    for(j in (N-1):2){
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 waight matrices length(Wn)==2
      ## The i intermediate Layers Backpropagation:
      derivativeWeights <- crossprod(Hx_function[[j-1]],dhidden) 
      derivativeBias <- colSums(dhidden)
      derivativePrelu <- sum(colSums(dPrelu))
      adagradW <- lr / sqrt(mean(derivativeWeights^2)+epsilon)#%*%derivativeWeights
      adagradB <- lr / sqrt(mean(derivativeBias^2)+epsilon)
      adagradA <- (lr / sqrt(mean(colSums(dPrelu)^2)+epsilon))
      
      #Calculate Delta for the next gradient computation:
      dhidden <- tcrossprod(dhidden,Wn[[j]]$W)
      dPrelu <- pmin(dhidden,0)
      dhidden[Hx_function[[j-1]] <= 0] <- Wn[[j-1]]$A * dhidden[Hx_function[[j-1]] <= 0]
      
      # Updating of Weights, Bias and Alfa-PReLu value:
      Wn[[j]]$W <- Wn[[j]]$W - adagradW * derivativeWeights
      Wn[[j]]$b <- Wn[[j]]$b - adagradB * derivativeBias
      Wn[[j]]$A <- 0.9 * Wn[[j]]$A + adagradA * derivativePrelu # 0.9:momentum of PReLu Ai parameter
    }
    ## Backpropagation for the Input Layer:
    derivativeWeights <- crossprod(X,dhidden)
    derivativeBias <- colSums(dhidden)
    derivativePrelu <- sum(colSums(dPrelu))
    # Updating parameters of the Input layer
    adagradW <- (lr / sqrt(mean(derivativeWeights^2)+epsilon))#%*%derivativeWeights
    adagradB <- (lr / sqrt(mean(derivativeBias^2)+epsilon))
    adagradA <- (lr / sqrt(mean(colSums(dPrelu)^2)+epsilon))
    Wn[[1]]$W <- Wn[[1]]$W - adagradW * derivativeWeights
    Wn[[1]]$b <- Wn[[1]]$b - adagradB * derivativeBias
    Wn[[1]]$A <- 0.9 * Wn[[1]]$A + adagradA * derivativePrelu # 0.9:momentum of PReLu Ai parameter
  }
  #############################################
  # Final results, statistics and memory release #
  cat("Total Data Loss MSE:", loss, "\n")
  residuals <- Y - y_DNNoutput
  res.std <- (residuals - mean(residuals))/sd(residuals)
  dev.set(1)
  plot(y_DNNoutput,res.std,
       xlab = "Predicted Values", ylab = "Standarized Residuals",
       main = "Residuals Plot vs Predicted Values")
  mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),sep = " "))
  abline(0,0)
  # Returning the trained model (Wn)
  model@dnn <- Wn
  model@error <- loss
  model@bestDnn <- bestModel
  model@bestError <- bestLoss
  if(is.null(bestModel2)){model@bestDnn2 <- as.list(bestModel2)}
  else{model@bestDnn2 <- bestModel2}
  model@bestError2 <- bestLoss2
  rm(Wn,bestModel,dhidden,Hx_function,X,Y)
  gc()
  return(model)
}

# ## WRAPPER FOR GENE-TARGET STANDARIZATION
deepNeuralNetwork.standarizegenescore <- function(x = NULL, method = "gene",gene.list = NA){
  
  if(!method %in% c("s","r","gene","genecomb")){
    message("Invalid Method \"",method,"\": \n - \"s\" for standar Z-score using the mean and SD \n - \"r\" for robust Z-score using the median and MAD \n - \"gene\" for specific-gene Z-score using the gene as centroid - |n \"genecomb\" for specific-gene Z-score using the gene as centroid and  sample-gene combined standar deviation")
    warning("choose method = \"-\"",immediate. = T)
  }
  
  if(is.null(x))stop("Standarizing data: ERROR, data is NULL.")
  if(!is.numeric(x))stop("Standarizing data: ERROR, trying to standarize non-numeric data.")
  if(is.null(dim(x)))x <- as.data.frame(x)
  
  standarization = gene.list
  new.data <- as.matrix(plyr::ldply(lapply(X = standarization,function(gene){
    
    if(is.na(gene))stop("Gene-target for standarization not specified.")
    message("Standarizing data using *specific-gene Z-score with MAD...\n",appendLF = F)
    MX.standarized <- apply(X = x,MARGIN = 2,FUN = function(y){
    sample.MAD <- mad(y)
    target.gene.exprs <- y[match(gene,names(y))]
    y <- y[-match(gene,names(y))]
    return((y - target.gene.exprs)/sample.MAD)})
    return(MX.standarized)
  }), data.frame))
  
  message("Standarizing data [by column] using *median absolute deviation(MAD) Z-score...\n",appendLF = F)
  rv <- apply(x,2,function(z){(z - median(z)) / mad(z)})
  new.data <- as.matrix(rbind(rv,new.data))
  message(" Completed.")
  return(new.data)
}
#

### Standarizing gene signal
deepNeuralNetwork.standarize <- function(x = NULL, method = "r",gene = NA){
  if(is.null(x))stop("Standarizing data: ERROR, data is NULL.")
  if(!is.numeric(x))stop("Standarizing data: ERROR, trying to standarize non-numeric data.")
  if(is.null(dim(x)))x<-as.data.frame(x)
  if(!method %in% c("s","r","gene","genecomb")){
    message("Invalid Method \"",method,"\": \n - \"s\" for standar Z-score using the mean and SD \n - \"r\" for robust Z-score using the median and MAD \n - \"gene\" for specific-gene Z-score using the gene as centroid - |n \"genecomb\" for specific-gene Z-score using the gene as centroid and  sample-gene combined standar deviation")
    warning("choose method = \"-\"",immediate. = T)
  }
  ## standard Z-Score using mean and SD:
  if(method == "s"){
    message("Standarizing data [by row] using *mean standar deviation Z-score...",appendLF = F)
    MX.standarized <- t(apply(X = x,MARGIN = 1,FUN = function(y){
      (y - mean(y))/sd(y)}))
    message(" Completed.\n")
    return(MX.standarized)
  } 
  #
  if(method == "r"){
    message("Standarizing data [by column] using *median absolute deviation(MAD) Z-score...",appendLF = F)
    rowmed <- apply(x, 2, median)
    rowmad <- apply(x, 2, mad)  # median absolute deviation
    rv <- (x - rowmed) / rowmad
    message(" Completed.")
    return(rv)
  }
}

deepNeuralNetwork.save <- function(wn = NULL,
                                   nameoffile = paste("DNN.model.",format(Sys.time(),"%d.%m.%Y.%H.%M.%S"),sep = "")){
  
  assign(nameoffile,wn)
  save(nameoffile,file = paste(nameoffile,".RData",sep = ""))
  
  return(nameoffile)
}

# 
# ###########################################################################################
# #       END OF SCRIPT
# #########################################################################################
# 
### 
# 
# ############################################
