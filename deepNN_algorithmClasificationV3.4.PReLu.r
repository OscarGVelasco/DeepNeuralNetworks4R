# Version 3.4 with SoftMax Function as loss clasification function
# **Activation function: PReLu -Parametric Rectifier Linear Unit

# Optimized version 2017-Jun-27 by Óscar González Velasco
# Deep Neural Network With multiple hidden Layers

########################################################################
### Build the Neural Network, values:
#
deepNeuralNetwork.build <- function(x,y,
                                    HidenLayerNeurons = c(4,4), inputNeurons = 0,
                                    outputNeurons = 0,Ai = 0.25,traindata=data,
                                    drawDNN = 1, random.seed = 1, standarization = NULL){
  source(file = "/home/oscar/Escritorio/deepNN/drawDeepNN.r")
  source(file = "/home/oscar/Escritorio/deepNN/ROC.plot.r")
  library(plyr)
  message(" Loading DNN model parameters...")
  Sys.sleep(1)
  # to make the case reproducible.
  set.seed(random.seed)
  if(!is.numeric(HidenLayerNeurons) || any(HidenLayerNeurons == 0,na.rm = T) || any(is.na(HidenLayerNeurons))){
    stop("Hiden Layer Neurons specification is not valid.")
    return(NULL)
  }
  if(is.list(standarization)){
  x <- 1:(length(x)*length(standarization))
  }
  # total number of training set
  N <- nrow(traindata)
  # extract the data and label
  # create model
  # number of input features
  ## D = Number of Neurons in input layer
  if(inputNeurons == 0){
    D <- length(x)
  }else{
    D <- inputNeurons  
  }
  input.layer.N.ratio <- 0.3
  second.layer.N.ratio <- 0.18
  if(D > 10){
    message("* Sugested topology for the Deep Neural Network: ")
    message("* Input Layer Neurons:",round(D*input.layer.N.ratio,digits = 0),
            " Second Layer Neurons: ",round(D*input.layer.N.ratio*second.layer.N.ratio,digits = 0))
    Sys.sleep(1)
  }
  # number of categories for classification
  ## K = Number of Neurons in output layer
  if(outputNeurons == 0){
    K <- length(unique(traindata[,y]))
  }else{
    K <- outputNeurons
  }## H = Number of Neurons in the hidden layers
  H <-  HidenLayerNeurons
  message("\n","\tDeep Neural Network Parameters:")
  Sys.sleep(1)
  message(" Input Layer: ",D," neurons]",appendLF = F)
  Sys.sleep(1)
  sapply(H,function(x){
    message("---[",x," neurons]",appendLF = F)
    Sys.sleep(1)})
  message("---[Output Layer: ",K," neurons.","\n")
  # create and init weights and bias 
  ## On weight Matrices Wi the Nrows corresponds to the number
  ## of Neurons on layer i and the Ncols corresponds to the number 
  ## of Neurons on layer i+1 (the layer where the data is flowing to)
  ## Initial values are randomly generated by rnorm generates random deviates with mean=0.
  ## we will apply a reduction factor 0.01 to obtain small weights
  Sys.sleep(2)
  message(" Initializing DNN weights and bias using a gaussian distribution...",appendLF = F)
  Sys.sleep(1)
  # Initialize weights with a gaussian distribution
  Winput <- 0.01*matrix(rnorm(D*H[1],sd = sqrt(2.0/D)), nrow=D, ncol=H[1])
  ## On bias matrices bi the Ncols corresponds to the number of Neurons on
  ## layer i+1
  binput <- matrix(0, nrow=1, ncol=H[1])
  A <- matrix(Ai, nrow=1, ncol=H[1])
  d <- list(W = Winput, b = binput, A = Ai)
  Wn <- list(d)
  Whl <- list()
  if(length(H)!=1){
    Whl <- sapply((1:(length(H)-1)),function(i){
      neuronLayer <- H[i]
      neuronLayerNext <- H[i+1]
      Wi <- 0.01*matrix(rnorm(neuronLayer*neuronLayerNext,sd = sqrt(2.0/neuronLayer)), nrow=neuronLayer, ncol=neuronLayerNext)
      bi <- matrix(0, nrow=1, ncol=neuronLayerNext)
      A <- matrix(Ai, nrow=1, ncol=neuronLayerNext)
      d <- list(W = Wi, b = bi, A = Ai)
      Whl[[i+1]] <- list(d)
      })
    for(i in (2:(length(Whl)+1))){
      Wn[[i]] <- Whl[[i-1]]
    }}
  remove(Whl)
  
  Woutput <- 0.01*matrix(rnorm(H[length(H)]*K, sd = sqrt(2.0/H[length(H)])), nrow=H[length(H)], ncol=K)
  boutput <- matrix(0, nrow=1, ncol=K)
  A <- matrix(Ai, nrow=1, ncol=K)
  Wn[[length(Wn)+1]] <- list(W = Woutput,b = boutput, A = Ai)
  message(" Done.")
  ### Drawing the Neural Network
  if(drawDNN == 1)draw.dnn(NperL = c(D,H,K))
  Sys.sleep(1)
  message(" Returning DNN model.")
  return(Wn)
}

#######################################
## Prediction - faster function for checking testdata prediction performance
deepNeuralNetwork.run <- function(model.trained, data = X.test) {
  # new data, transfer to matrix
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  new.data <- data.matrix(data)
  Wn <- model.trained
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(new.data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }}
  
  # Loss Function: softmax
  score.exp <- exp(y_output)
  probs <-sweep(score.exp, 1, rowSums(score.exp), '/') 
  
  # select and return max possiblity
  return(max.col(probs))
}

deepNeuralNetwork.predict <- function(model.trained, data = X.test, standarization = NULL) {
  # new data, transfer to matrix
  # Activation Function: PReLu (Parametric Rectifier Linear unit)
  #       - PReLu avoids 0 gradient minimum by applying Ai*y when y<0
  message(" Standarizing Data...",appendLF = F)
  if(is.list(standarization)){
    new.data <- t(data)
    new.data <- t(ldply(lapply(X = standarization,function(x)standarize(x = new.data,method = "genecomb",gene = as.character(x))), data.frame))
  }else{
    new.data <- deepNeuralNetwork.standarize(data.matrix(data))
  }
  Sys.sleep(time = 1)
  message(" Done.")
  Sys.sleep(time = 1)
  message(" Loading model...",appendLF = F)
  Wn <- model.trained
  Sys.sleep(time = 1)
  message(" Done.")
  Sys.sleep(time = 1)
  N <- length(Wn)
  message("\n\t Deep Neural Network Topology:")
  message("> Number of layers: ",N)
  sapply(1:N,function(x){
    message("[Layer ",x," Neurons:",nrow(Wn[[x]]$W),"]--",appendLF = F)
    Sys.sleep(time = 1)})
  message("[Output layer Neurons:",ncol(Wn[[N]]$b),"]")
  Sys.sleep(time = 1)
  Hx <- list() # Hx[[l]]:Output of layer l 
  # Feed Forwad with the DataSet through the NN
  for(i in 1:length(Wn)){
    if(i == 1){
      Hx[[i]] <- sweep(new.data %*% Wn[[i]]$W ,2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }else if(i == length(Wn)){
      y_output <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
    }else{
      Hx[[i]] <- sweep(Hx[[i-1]] %*% Wn[[i]]$W, 2, Wn[[i]]$b, '+')
      # Activation Function: PReLu Parametric Rectifier Linear
      Hx[[i]] <- pmax(Hx[[i]], Wn[[i]]$A * Hx[[i]])
    }}
  
  # Loss Function: softmax
  score.exp <- exp(y_output)
  probs <-sweep(score.exp, 1, rowSums(score.exp), '/') 
  
  # select max possiblity
  r <- max.col(probs)
  message("\n> Prediction Completed. Returning results...")
  return(r)
}

### Standarizing gene signal by targeted gene ENSG00000075856 stable in Hippocampus
### Standarizing gene signal by targeted gene ENSG00000124214 stable in Cortex
deepNeuralNetwork.standarize <- function(x = NULL, method = "gene",gene = "ENSG00000075856") {
  if(is.null(x))stop("Standarizing data: ERROR, data is NULL.")
  if(!is.numeric(x))stop("Standarizing data: ERROR, trying to standarize non-numeric data.")
  if(is.null(dim(x)))x<-t(as.data.frame(x))
  if( (summary(apply(x,MARGIN = 1,FUN = sd))[3] == 1) && (summary(apply(x,MARGIN = 1,FUN = sd))[4]== 1) ){
    warning("The Data is already standarized (Standar Deviation = 1) \n*None standarization applied.")
    return(x)}
  if(!method %in% c("s","r","gene","genecomb")){
    message("Invalid Method \"",method,"\": \n - \"s\" for standar Z-score using the mean and SD \n - \"r\" for robust Z-score using the median and MAD \n - \"gene\" for specific-gene Z-score using the gene as centroid - |n \"genecomb\" for specific-gene Z-score using the gene as centroid and  sample-gene combined standar deviation")
    warning("choose method = \"-\"",immediate. = T)
  }
  ## standard Z-Score using mean and SD:
  if(method == "s"){
    message("Standarizing data [by row] using *mean standar deviation Z-score...",appendLF = F)
    MX.standarized <- t(apply(X = x,MARGIN = 1,FUN = function(y){
      (y - mean(y))/sd(y)}))
    message(" Completed.\n")
    return(MX.standarized)
  } 
  #
  if(method == "r"){
    message("Standarizing data [by row] using *median absolute deviation(MAD) Z-score...",appendLF = F)
    rowmed <- apply(x, 1, median)
    rowmad <- apply(x, 1, mad)  # median absolute deviation
    rv <- sweep(x, 1, rowmed,"-")  #subtracting median expression
    rv <- sweep(rv, 1, rowmad, "/")  # dividing by median absolute deviation
    message(" Completed.")
    return(rv)
  }
  if(method == "gene"){
    message("Standarizing data [by column] using *specific-gene Z-score...",appendLF = F)
    MX.standarized <- apply(X = x,MARGIN = 2,FUN = function(y){
      (y - y[match(gene,names(y))])/sd(y)})
    message(" Completed.\n")
    return(MX.standarized)
    # rv <- apply(x,1,function(x){x-x[gene]})
    #rv <- sweep(x, 1, gene.signal,"-")  #subtracting median expression
    # rv <- sweep(rv, 1, gene.sd, "/")  # dividing by median absolute deviation
  }
  if(method == "genecomb"){
    message("Standarizing data using *specific-gene Z-score with SD(sample,gene) combined...",appendLF = F)
    gene.mean <- mean(x[match(gene,rownames(x)),])
    gene.n <- length(x[match(gene,rownames(x)),])
    gene.sd <- sd(x[match(gene,rownames(x)),])
    MX.standarized <- apply(X = x,MARGIN = 2,FUN = function(y){
      sample.mean <- mean(y)
      sample.n <- length(y)
      sample.sd <- sd(y)
      combined.mean <- ((gene.n*gene.mean)+(sample.n*sample.mean))/(gene.n+sample.n)
      combined.sd <- sqrt( ( ((gene.n-1)*(gene.sd^2))+((sample.n-1)*(sample.sd^2))+gene.n*((gene.mean-combined.mean)^2)+sample.n*((sample.mean-combined.mean)^2) )/ (gene.n+sample.n-1))
      return((y - y[match(gene,names(y))])/combined.sd)})
    message(" Completed.\n")
    return(MX.standarized)
  }
}

##################################################
# Training the Deep Neural Network built by build.dnn function, a dataset must be
#    passed in order to train the DNN, x= columns of the data , y = columns of known results that can be predicted using x
deepNeuralNetwork.training <- function(x,y, model = NULL,
                                       traindata=data,
                                       testdata=NULL,
                                       # max iteration steps
                                       iterations=2000,
                                       # delta loss 
                                       minError=1e-2,
                                       # learning rate
                                       lr = 1e-2,
                                       # regularization rate
                                       reg = 1e-3,
                                       # show results every 'display' step
                                       display = 100,
                                       random.seed = 1,
                                       # Switch data between traindata and testdata to prevent overfitting?
                                       switch.data = FALSE,
                                       # Normalization method/vector of target genes to standarize upon
                                       standarization = NULL)
{
  # # ### TEST
  # x=1:4
  # y=5
  # model = model
  # traindata=iris
  # testdata=iris
  # iterations=2000
  # lr = 1e-2
  # reg = 1e-3
  # ####
  # to make the case reproducible.
  set.seed(random.seed)
  decision.function <- "ReLu: Rectifier Linear Unit"
  # total number of training set
  N.data <- nrow(traindata)
  # This X Data will be the explanatory data 
  X.raw <- data.matrix(traindata[,x])
  # X.alfa <- t(deepNeuralNetwork.standarize(t(X.raw)))
  if(is.list(standarization)){
  X.raw.t <- t(X.raw)
  X.alfa <- t(ldply(lapply(X = standarization,function(x)standarize(x = X.raw.t,method = "genecomb",gene = as.character(x))), data.frame))
  }else{X.alfa <- t(deepNeuralNetwork.standarize(t(X.raw)))}
  # correct categories represented by integer
  # This Y data will be the data to be predicted: Y = F(X) + a
  Y.alfa <- traindata[,y]
  if(is.factor(Y.alfa)) { Y.alfa <- as.integer(Y.alfa) }
  # create index for both row and col
  Y.len   <- length(unique(Y.alfa))
  # len is the number of different values that Y can take
  Y.set   <- sort(unique(Y.alfa))
  Y.index <- cbind(1:N.data, match(Y.alfa, Y.set))
  
  # Test data for checking overfitting
  if(is.null(testdata)){
    warning(" - - No testdata detected - - \n> Training DNN without alternative data. Overfitting is not being checked.")
  }else{
    N.prima.data <- nrow(testdata)
    # X.prima <- t(deepNeuralNetwork.standarize(t(testdata[,x])))
    if(is.list(standarization)){
      X.prima.t <- t(testdata[,x])
      X.prima <- t(ldply(lapply(X = standarization,function(x)standarize(x = X.prima.t,method = "genecomb",gene = as.character(x))), data.frame))
    }else{X.prima <- t(deepNeuralNetwork.standarize(t(testdata[,x])))}
    Y.prima <- as.integer(testdata[,y])
    Y.prima.len   <- length(unique(Y.prima))
    # len is the number of different values that Y can take
    Y.prima.set   <- sort(unique(Y.prima))
    Y.prima.index <- cbind(1:N.prima.data, match(Y.prima, Y.prima.set))
  }
  # The model list with the Weights and Biases
  Wn <- model
  # use all train data to update weights since it's a small dataset
  batchsize <- N.data
  # Loss error and pastLoss initialization value 
  loss <- 50000
  data.loss <- 0
  pastLoss <- 50001
  bestAcuracy <- 0
  bestModel <- NULL
  bestLoss <- -1
  predictionLoss <- 50000
  partialPredict <- 50000
  # Y value dispersion index SST
  SST <- sum((Y.alfa - mean(Y.alfa))^2)
  ## epsilon: Adagrad value to prevent division by 0
  epsilon <- 1e-8
  message("\n DNN model loaded with parameters:")
  message("\t * Learning Rate: ",lr)
  message("\t * Iterations: ",iterations)
  message("\t * Decision Function: ",decision.function)
  message("\n")
  # Training the network
  i <- 0
  Hx_function <- list()
  N <- length(Wn)
  X <- X.alfa
  Y <- Y.alfa
  X.next <- X.prima
  Y.next <- Y.prima
  Y.index.next <- Y.prima.index
  acurracy <- 0
  bestTestAcuracy <- 0
  partialPredict <- 0
  X11()
  while(i < iterations) { # && loss > minError ) {
    # iteration index
    i <- i +1
    if( (i %% display == 0) && (switch.data == T) && (acurracy > 0.9) && ((acurracy-partialPredict)>0.05)){
      message(" Switching: Training Data set <--> Testing Data Set")
      X.tmp <- X
      Y.tmp <- Y
      Y.index.tmp <- Y.index
      
      X <- X.next
      Y <- Y.next
      Y.index <- Y.index.next
      
      X.next <- X.tmp
      Y.next <- Y.tmp
      Y.index.next <- Y.index.tmp
    }
    
    ## Calculation of the very first layer (INPUTDATA x WEIGHTMatrix + BIAS)
    Hx_function[[1]] <- sweep(X %*% Wn[[1]]$W ,2, Wn[[1]]$b , '+')
    # PReLu Function
    Hx_function[[1]] <- pmax(Hx_function[[1]], Wn[[1]]$A * Hx_function[[1]])
    for(j in 2:(N-1)){
      # forward ....
      # 1 indicate row, 2 indicate col
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 waight matrices length(Wn)==2
      # neurons : ReLU
      # pmax compares parallely each row with a row of 0's
      # in this case, if a value is <0 it will be set to 0
      # this is a fast way to compute the ReLU function (f(x)= max(0,x))
      Hx_function[[j]] <- sweep(Hx_function[[j-1]] %*% Wn[[j]]$W ,2, Wn[[j]]$b , '+')
      # PReLu Function
      Hx_function[[j]] <- pmax(Hx_function[[j]], Wn[[j]]$A * Hx_function[[j]])
    }
    y_DNNoutput <- sweep(Hx_function[[N-1]] %*% Wn[[N]]$W, 2, Wn[[N]]$b, '+')
    #####################################################
    # output function to clasify labels:
    # LOSS FUNCTION:
    # Softmax: function to map Scores to labels with a probability 
    if(any(is.infinite(score.exp <- exp(y_DNNoutput)))){
      warning("Infinites generated on SoftMax function. Check Learning Rate: ",lr)
      message("Iteration: ",i)
      message("Returning last best stable model.")
      return(list(dnn = Wn, error = acurracy, bestDnn = bestModel, bestError = bestAcuracy))
    }
    probs <- score.exp/rowSums(score.exp)
    # debug
    # compute the loss
    corect.logprobs <- -log(probs[Y.index])
    labels.predicted <- max.col(probs)
    
    # SSE <- sum(probs^2)
    # Epredicted <- SSE/SST
    acurracy <- mean(as.integer(Y) == as.integer(labels.predicted))
    reg.loss   <- 0.5*reg* sum(sapply(1:N,function(x){sum(Wn[[x]]$W*Wn[[x]]$W)}))
    loss <- data.loss + reg.loss
    
    
    
    if( i %% 100 == 0){
    if(!is.null(testdata))partialPredict <- round(mean(deepNeuralNetwork.run(model.trained = Wn,
                                                         data = X.next) == Y.next),digits = 2)
    if(((bestAcuracy <= acurracy)) && (bestTestAcuracy < partialPredict)){
      bestAcuracy <- acurracy
      bestTestAcuracy <- partialPredict
      bestModel <- Wn
    }}
    
    ########################
    ## Print partial results
    if( i %% display == 0){
      message("#####################################################")
      message("\r","> Iteration Number: ",i) 
      message("> Actual acuracy on the training data set clasification: ",acurracy)
      #######################################
      ## Check for Overfitting and early stop
      if(!is.null(testdata)){
        partialResult <- deepNeuralNetwork.run(model.trained = Wn,
                                               data = X.next)
        percent <- round((i/iterations)*100,digits = 1)
        partialPredict <- round(mean(partialResult == Y.next),digits = 2)
        message("\t*Acuracy on alternative data: ",partialPredict)
        message("\t*Best Acuracy achieved: ",bestAcuracy)
        # debug
        message("\t| ",percent,"% completed |")
        message("##################################################### \n")
        #flush.console()
        model_name = "Deep Neural Network Clasification"
        subtitle = paste("Iteration: ",i," [",percent,"%] - Acuraccy: ",round(acurracy,digits = 2)," - Acuraccy on testset: ",partialPredict)
        print(mplot_roc(tag = Y,score = labels.predicted,model_name = model_name,subtitle = subtitle))
        #print(mplot_roc(tag = Y,score = probs[Y.index],model_name = model_name,subtitle = subtitle))
        #print(mplot_density(tag = Y,score = labels.predicted))
        #qqplot(labels.predicted,Y, xlab = "Predicted", ylab = "Observed", main = "Observed Values vs Predicted Values")
        #mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),"Acurracy:",as.character(mean(as.integer(Y) == as.integer(labels.predicted))),sep = " "))
        #abline(0,1)
      }
    }else{
      # if( i %% 200 == 0){
      #   message("| ",round((i/iterations)*100,digits = 1),"% completed |")
      #   flush.console()
      #  }
    }
    ############################
    # Backpropagation Algorithm: updating of Weights and Bias
    # We start by updating and calculating the values for the output layer -> j layer -> inputLayer
    delta_y <- probs
    
    ## ERROR CALCULATION ##
    # 1 -> 100% probability of the result being that particular label
    # We caculate de difference of the probability given to the true label:
    # Y.observed - 1 (the value that we want to reach, that is, 100% probability of being that class)
    delta_y[Y.index] <- delta_y[Y.index] - 1
    delta_y <- delta_y / batchsize
        
    # Wn[[N]] Here corresponds with layer l-1 weights
    dhidden <- tcrossprod(delta_y,Wn[[N]]$W)
    ## Output Layer (Layer N-1)
    derivativeWeights <- crossprod(Hx_function[[N-1]],delta_y) 
    derivativeBias <- colSums(delta_y)
    dPrelu <- pmin(dhidden,0) # Applying the derivative ReLu 
    
    # Applying the derivative ReLu 
    dhidden[Hx_function[[N-1]] <= 0] <- Wn[[N-1]]$A * dhidden[Hx_function[[N-1]] <= 0]
    
    Wn[[N]]$W <- Wn[[N]]$W - lr * derivativeWeights
    Wn[[N]]$b <- Wn[[N]]$b - lr * derivativeBias
    # ## NO Wn$A on N Layer as there is no activation function on
    #     the last layer, only loss function
    
    for(j in (N-1):2){
      if(N==2)break()
      ## Special case contemplating the NN of only 3 layers (minimun Nº of layers)
      ## on a 3-layer NN there will be only 2 waight matrices length(Wn)==2
      ## The i intermediate Layers Backpropagation:
      derivativeWeights <- crossprod(Hx_function[[j-1]],dhidden) 
      derivativeBias <- colSums(dhidden)
      derivativePrelu <- sum(colSums(dPrelu))
      
      #Calculate Delta for the next gradient computation:
      dhidden <- tcrossprod(dhidden,Wn[[j]]$W)
      dPrelu <- pmin(dhidden,0)
      dhidden[Hx_function[[j-1]] <= 0] <- Wn[[j-1]]$A * dhidden[Hx_function[[j-1]] <= 0]
      
      # Updating of Weights, Bias and Alfa-PReLu value:
      Wn[[j]]$W <- Wn[[j]]$W - lr * derivativeWeights
      Wn[[j]]$b <- Wn[[j]]$b - lr * derivativeBias
      Wn[[j]]$A <- 0.9 * Wn[[j]]$A + lr * derivativePrelu # 0.9:momentum of PReLu Ai parameter
    }
    ## Backpropagation for the Input Layer:
    derivativeWeights <- crossprod(X,dhidden)
    derivativeBias <- colSums(dhidden)
    derivativePrelu <- sum(colSums(dPrelu))
    # Updating parameters of the Input layer
    Wn[[1]]$W <- Wn[[1]]$W - lr * derivativeWeights
    Wn[[1]]$b <- Wn[[1]]$b - lr * derivativeBias
    Wn[[1]]$A <- 0.9 * Wn[[1]]$A + lr * derivativePrelu # 0.9:momentum of PReLu Ai parameter
    
  }
  #############################################
  # Final results, statistics and free memory #
  
  remove(Hx_function)
  residuals <- Y - y_DNNoutput
  res.std <- (residuals - mean(residuals))/sd(residuals)
  #dev.set(1)
  # plot(y_DNNoutput,res.std,
  #      xlab = "Predicted Values", ylab = "Standarized Residuals",
  #      main = "Residuals Plot vs Predicted Values")
  # mtext(paste("Iteration:",i,"Loss:",round(loss, digits = 4),sep = " "))
  # abline(0,0)
  # # Returning the trained model (Wn)
  results <- list(dnn = Wn, error = acurracy, bestDnn = bestModel, bestError = bestAcuracy)
  return(results)
}

deepNeuralNetwork.save <- function(wn = NULL,
                                   nameoffile = paste("DNN.model.",format(Sys.time(),"%d.%m.%Y.%H.%M.%S"),sep = "")){
  
  assign(nameoffile,wn)
  save(list = nameoffile,file = paste(nameoffile,".RData",sep = ""))
  
  return(nameoffile)
}
########################################################################
# testing
#######################################################################
set.seed(1)
# # 0. EDA
# summary(iris)
# plot(iris)ñ
# 1. split data into test/train
samp <- c(sample(1:50,25), sample(51:100,25), sample(101:150,25))
# 2. Create the Neural Network specifying the layers
model <- deepNeuralNetwork.build(x=1:4,y=5,HidenLayerNeurons = c(100,5),
                                 traindata=iris[samp,],random.seed = 1, drawDNN = 0)

# 3. train model
timeNN <- system.time(
  model.trained <- deepNeuralNetwork.training(x=1:4,y=5, model = model, traindata=iris[samp,],
                                              testdata=iris[-samp,], lr = 0.01, reg = 0.001
                                              ,iterations = 10000, display=1000))
# 4. prediction
# NOTE: if the predict is factor, we need to transfer the number into class manually.
#       To make the code clear, I don't write this change into predict.dnn function.
results <- deepNeuralNetwork.run(model.trained = model.trained$bestDnn,
                                 data = iris[-samp, -5])
# Total Data Loss MSE: 0.2946946 with 10 neurons and 4000 iterations
# 5. verify the results
#table(iris[-samp,1], results)
table(iris[-samp,5], results)
accuracy <- mean((as.integer(iris[-samp,5])==results))
mean(as.integer(Y) == as.integer(labels.predicted))
as.numeric(iris[-samp,1])[1:10] - results[1:10]
cor(as.numeric(iris[-samp,1]),results)
sqrt(mean((as.numeric(iris[-samp,1]) - results)^2))
#0.6773504
#          labels.dnn
#            1  2  3
#setosa     25  0  0
#versicolor  0 24  1
#virginica   0  0 25
# 6. accuracy
mean(as.integer(iris[-samp, 5]) == results)
# 0.98



############################################
## Calculating AD STAGE with expression data
setwd("/home/oscar/Escritorio/deepNN/")
load(file = "ADinterrogation_CodingrmaMX.RData")
load(file = "ADinterrogation_Phenodata.RData")
load(file = "ADhokama.MX.RData")
load(file = "ADhokama_Phenodata.RData")
load(file = "UKBEC_CodingrmaMX.RData")
setwd("/home/oscar/Escritorio/Ad_definitive_results_6Sep2018/hip_zscored_signal/")
zscore.cortex.MX <- read.table(file= "liang.hokama.berchtol.zscored.withnormalaverage.mastertable.HIPPOCAMPUS.corrected.txt",header = T,sep = "\t",stringsAsFactors = F)
rownames(zscore.cortex.MX) <- zscore.cortex.MX$X
zscore.cortex.MX <- zscore.cortex.MX[,-1]
zscore.cortex.MX <- zscore.cortex.MX[order(zscore.cortex.MX$p.val.combined.adj,decreasing = F),]
#
#targetGenes <- ad.intersections.hipocampus[ad.intersections.hipocampus[,1] %in% rownames(ADinterrogation_CodingrmaMX),1]
targetGenes <- rownames(zscore.cortex.MX)
targetGenes <- targetGenes[targetGenes %in% rownames(ADhokama.MX)]
targetGenes <- targetGenes[targetGenes %in% rownames(UKBEC_CodingrmaMX)]
targetGenes <- c(targetGenes[1:300],"ENSG00000075856")
### Standarizing gene signal by targeted gene ENSG00000075856 stable in Hippocampus
#
genes.zscore <- as.list(targetGenes[299:301])
samp2 <-sample(1:161,80)
data <- t(ADinterrogation_CodingrmaMX[targetGenes,samp2])
data <- cbind(data, ad= ADinterrogation_Phenodata[rownames(data),]$disease_state)
testdata <- t(ADinterrogation_CodingrmaMX[targetGenes,-samp2])
testdata <- cbind(testdata, ad= as.factor(ADinterrogation_Phenodata[rownames(testdata),]$disease_state))

model <- deepNeuralNetwork.build(x=1:(ncol(data)-1),y=ncol(data),
                                 HidenLayerNeurons = c(100,20,20,20,20,20,10,5),traindata=data,
                                 random.seed = 1, drawDNN = 0,standarization = genes.zscore)
# 3. train model
timeNN <- system.time(
  model.trained <- deepNeuralNetwork.training(x=1:(ncol(data)-1),y=ncol(data), model = model,
                                              traindata=data, testdata=testdata, 
                                              iterations  = 60000, lr = 0.01, 
                                              reg = 0.0001, display=1000,switch.data = T,standarization = genes.zscore))
predictData <- t(ADinterrogation_CodingrmaMX[targetGenes,-samp2])
#predictData <- cbind(predictData, ad= ADinterrogation_Phenodata[rownames(predictData),]$disease_state)
results <- deepNeuralNetwork.predict(model.trained = model.trained$bestDnn, data = predictData, standarization = genes.zscore)
ad <- as.numeric(ADinterrogation_Phenodata[rownames(predictData),]$disease_state)

table(results,ad)
mean(results == ad)
print(mplot_roc(tag = ad,score = results))

#####

ADhokama.MX
samp3 <-sample(1:ncol(ADhokama.MX),ncol(ADhokama.MX)/2)
predictData2 <- t(ADhokama.MX[targetGenes,samp3])

results2 <- deepNeuralNetwork.predict(model.trained = model.trained$bestDnn, data = predictData2, standarization = genes.zscore)
ad2 <- as.numeric(factor(ADhokama_Phenodata[rownames(predictData2),]$AD))
table(results2,ad2)
mean(results2 == ad2)
print(mplot_roc(tag = ad2,score = results2))



###### Mixed Data for training
samp2 <-sample(1:161,80)
data <- t(ADinterrogation_CodingrmaMX[targetGenes,samp2])
data <- cbind(data, ad= ADinterrogation_Phenodata[rownames(data),]$disease_state)
testdata <- t(ADinterrogation_CodingrmaMX[targetGenes,-samp2])
testdata <- cbind(testdata, ad= as.numeric(ADinterrogation_Phenodata[rownames(testdata),]$disease_state))

samp3 <-sample(1:ncol(ADhokama.MX),ncol(ADhokama.MX)/2)
data2 <- t(ADhokama.MX[targetGenes,samp3])
data2 <- cbind(data2, ad <- as.numeric(factor(ADhokama_Phenodata[rownames(data2),]$AD)))

data.mixed <- rbind(data,data2)

model <- deepNeuralNetwork.build(x=1:(ncol(data.mixed)-1),y=ncol(data.mixed),
                                 HidenLayerNeurons = c(2000,500,100,100),traindata=data.mixed,
                                 random.seed = 1, drawDNN = F)
# 3. train model
timeNN <- system.time(
  model.trained <- deepNeuralNetwork.training(x=1:(ncol(data.mixed)-1),y=ncol(data.mixed), model = model,
                                              traindata=data.mixed, testdata=testdata, 
                                              iterations  = 2220000, lr = 0.001, 
                                              reg = 0.0001, display=5000,switch.data = T))


predictData2 <- t(ADhokama.MX[targetGenes,-samp3])
results2 <- deepNeuralNetwork.predict(model.trained = model.trained$dnn, data = predictData2)
ad2 <- as.numeric(factor(ADhokama_Phenodata[rownames(predictData2),]$AD))
table(results2,ad2)
mean(results2 == ad2)
print(mplot_roc(tag = ad2,score = results2,model_name = "Deep Neural Network Clasification",subtitle = paste("Iteration: ",2000)))
mplot_density(tag = ADhokama_Phenodata[rownames(predictData2),]$AD,score = results2)
#######
# Trick using only controls
load("UKBEC_CodingrmaMX.RData")
results3 <- deepNeuralNetwork.predict(model.trained = model.trained$dnn, data = t(UKBEC_CodingrmaMX[targetGenes,]))
table(results3,rep(2,ncol(UKBEC_CodingrmaMX)))
mean(results3 == rep(2,ncol(UKBEC_CodingrmaMX)))
print(mplot_roc(tag = c(rep(2,ncol(UKBEC_CodingrmaMX)-3),1,1,1),score = results3,model_name = "Deep Neural Network Clasification",subtitle = paste("Iteration: ",2000)))

